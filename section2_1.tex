{ %section2_1
	\subsection{Parallel Acceleration and Parallel Efficiency}
	\par To evaluate the effectiveness of a parallel program, the performance indicators of this program are compared when it is run on several identical computing systems, which differ only in the number of central processors (or cores). In practice, several independent hardware platforms are rarely used for this purpose because it is quite difficult to ensure their full identity in all respects. Instead, measurements are performed on a single multiprocessor (multi-core) computing system, in which the number of processors (cores) involved in the calculations is artificially limited. This is usually achieved in one of the following ways:
	\begin{itemize}
		\item Setting the affinity of processors (cores).
		\item Virtualization of processors (cores).
		\item Controlling the number of threads of execution.
	\end{itemize}
	\textbf{Affinity setting.} By affinity (processor affinity/pinning) is meant the instruction to the operating system to run the specified thread/process on an explicitly specified processor (core). Affinity can be established either using a special system call from within the parallel program itself, or in some way from outside the parallel program (for example, using the Task Manager or using the start command with the / AFFINITY key in MS OS Windows, or the "taskset ' command on Linux). The disadvantages of this method are:
	\begin{itemize}
		\item The need to modify the parallel program under study when using a system call from the program itself..
		\item Inability to control affinity at the thread level, as usually, the OS allows affinity to be set only for processes when affinity is set by means external to the parallel program.
	\end{itemize}
	\textbf{Virtualization of processors (cores).} When creating a virtual computer in most specialized programs (for example, VMWare, \\VirtualBox) it is possible to "highlight ' the created virtual machine not all the processors (kernels) present in the host system, but only a part of them. This can be used to simulate a test environment with a given number of cores (processors). For example,  figure~\ref{VirtualBoxNumberCores:image} shows that for a custom virtual machine, of the eight available physical (and logical) processors, only three are available.
	\begin{figure}[H]
		\includegraphics[width=1\linewidth]{VirtualBoxNumberCores}
		\caption{\textit{Choosing the Number of Virtual Processors in Oracle VirtualBox}}
		\label{VirtualBoxNumberCores:image}
	\end{figure}
	\par The disadvantage of this approach is the virtualization overhead, which in an unpredictable way can affect the results of experimental measurements of parallel program performance. The advantage of virtualization (in comparison with controlled affinity) is a more natural behavior of the tested program when using available processors, because The OS does not give hard instructions that certain threads should always be `` tied '' to predefined processors (cores) - this feature allows you to more accurately reproduce the scenario of potential "live'' use of the program under test, which increases the reliability of the obtained performance measurements .
	\par\textbf{Thread management.} Quite often, the number of threads created during the work of a program is not set in the form of a rigidly fixed value. On the contrary, it is a flexibly configurable quantity p, the choice of the value of which allows optimal use of the computing resources of the hardware platform on which the program runs. This allows the program to "adapt'' to the number of processors (cores) that are available on a particular computer.
	\par This feature of a parallel program can be used to experimentally measure its performance indicators, for which a parallel program is started at $p = 1,2,…,n$, where $n$ –  is the number of available processors (cores) on the multiprocessor hardware platform used for testing. The described approach allows you to artificially limit the number of processors (cores) used in the program, because at any time, a parallel program can be executed no more than $p$ calculators. By analyzing the measurements of the program speed obtained for various $p$, it is possible to calculate the values of some indicators of parallelization efficiency (see below).
	\par\textbf{Parallel speedup.} In contrast to the concept of the value of acceleration used in physics as an increase in speed per unit time, in programming, parallel acceleration is understood to be a dimensionless quantity that reflects the increase in the speed of parallel program execution on a given number of processors compared to a single-processor system, i.e.
	\begin{equation}
		\label{parallelAcceleration:equation}
		S(p)\;=\;\frac{V(p)}{V(1)},
	\end{equation}
	where V (p) is the average speed of program execution on $p$ processors (cores), expressed in arbitrary units of work per second (W/s). Examples of W/S can be the number of summed matrix elements, the number of image points processed by the filter, the number of bytes written to the file, etc.
	\par It is believed that the value of $ S(p)$ can never exceed $p$, which on the intuitive level sounds plausible, because with an increase in the number of employees, for example, four times it is not possible to get the job done five times faster. However, as we consider below, in experiments, super-linear parallel acceleration may well be observed with an increase in the number of processors. Of course, such a result most often means an experimenter's mistake, however, there are situations where this result can be explained by the fact that with an increase in the number of processors, their computing resource not only multiplies, but also the volume of the first-level cache increases, which allows some tasks significantly increase the percentage of cache hits and, as a result, reduce the time to solve the problem.
	\par\textbf{Parallel efficiency.} Although the value of parallel acceleration is dimen-sionless, its analysis is not always possible without information on the value of $p$. For example, suppose in some experiment that $S(p) = 10 $. Without knowing the value of $p$, we can only say that with parallel execution the program began to work 10 times faster. However, if at the same time $ p = 1000 $, this acceleration cannot be considered a good achievement, since in other conditions, it was possible to achieve an almost 1000-fold increase in the speed of work and not spend such impressive resources on a poorly parallelized task. On the contrary, with a value of $ p = 11 $, one could consider the value $ S (p) = 10 $ to be quite acceptable.
	\par This problem led to the need to determine another indicator of the effectiveness of a parallel program, which would allow us to obtain some estimate of the parallelization efficiency taking into account the number of processors (cores). This quantity is
	 \textbf{parallel efficiency}
	\begin{equation}
		\label{parallelEffect:equation}
		E(p)\;=\;\frac{S(p)}p\;=\;\frac{V(p)}{p\cdot\;V(1)}
	\end{equation}
	\par Average program execution speed $V(p)$ can be measured by the following two \textit{nonequivalent} methods
	\begin{itemize}
		\item\textbf{Amdal Method:} calculate $V(p)$, fixing the amount of work performed (we change the execution time of the program for various).
		\item\textbf{Gustavson-Barsis Method:} calculate $V(p)$, fixing the time of the test program (we changes the amount of work done for different $ p $).
	\end{itemize}
Let us consider in more detail each of these methods in the next two sections.
	\par
}